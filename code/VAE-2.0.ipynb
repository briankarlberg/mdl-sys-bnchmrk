{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2cc8a-918c-4ee9-90c8-025d59e1b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE 2.0\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "# from scipy.spatial import distance  # Replace with TF cos sim\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "log_dir = Path(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "if not log_dir.exists():\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_type_encoder = LabelEncoder()\n",
    "cancer_type_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "# def calculate_cosine_similarity(df: pd.DataFrame, model_type_column='model_type') -> (dict, dict):\n",
    "#     # Separate the DataFrame into two based on the model_type\n",
    "#     df_cell_line = df[df[model_type_column] == 'cell line'].drop(columns=[model_type_column])\n",
    "#     df_tumor = df[df[model_type_column] == 'Tumor'].drop(columns=[model_type_column])\n",
    "\n",
    "#     # Initialize dictionaries to store the results\n",
    "#     cosine_similarities_cell_line = {}\n",
    "#     cosine_similarities_tumor = {}\n",
    "\n",
    "#     # Identify the columns that are common and valid for mean calculation\n",
    "#     valid_columns = df_cell_line.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "#     # Calculate the mean vector for each class for the relevant columns\n",
    "#     mean_vector_cell_line = df_cell_line[valid_columns].mean(axis=0).values\n",
    "#     mean_vector_tumor = df_tumor[valid_columns].mean(axis=0).values\n",
    "\n",
    "#     # Loop through each sample in cell_line and calculate cosine similarity to both mean vectors\n",
    "#     for index, row in df_cell_line.iterrows():\n",
    "#         sample_vector = row[valid_columns].values\n",
    "#         sim_to_cell_line = distance.cosine(sample_vector, mean_vector_cell_line)\n",
    "#         sim_to_tumor = distance.cosine(sample_vector, mean_vector_tumor)\n",
    "#         cosine_similarities_cell_line[index] = (sim_to_cell_line, sim_to_tumor)\n",
    "\n",
    "#     # Loop through each sample in tumor and calculate cosine similarity to both mean vectors\n",
    "#     for index, row in df_tumor.iterrows():\n",
    "#         sample_vector = row[valid_columns].values\n",
    "#         sim_to_cell_line = distance.cosine(sample_vector, mean_vector_cell_line)\n",
    "#         sim_to_tumor = distance.cosine(sample_vector, mean_vector_tumor)\n",
    "#         cosine_similarities_tumor[index] = (sim_to_cell_line, sim_to_tumor)\n",
    "\n",
    "#     # Combine the 2 dist dicts into one dict\n",
    "#     cosine_similarities_tumor.update(cosine_similarities_cell_line)\n",
    "#     cosine_similarities = cosine_similarities_tumor\n",
    "\n",
    "#     # Prefilled 0s lists, number of dictionary keys\n",
    "#     # Modified for truncated batch size (final batch of epoch)\n",
    "#     intra_cluster_tensor = list(range(df.shape[0]))\n",
    "#     inter_cluster_tensor = list(range(df.shape[0]))\n",
    "\n",
    "#     assert len(intra_cluster_tensor) == len(df), \"Length of list is not as expected\"\n",
    "#     assert len(inter_cluster_tensor) == len(df), \"Length of list is not as expected\"\n",
    "\n",
    "#     # Populate the zero lists with distance scores\n",
    "#     for key in cosine_similarities.keys():\n",
    "#         intra_cluster_tensor[key] = cosine_similarities[key][0]\n",
    "#         inter_cluster_tensor[key] = cosine_similarities[key][1]\n",
    "\n",
    "#     # Convert lists to tensors\n",
    "#     intra_cluster_tensor = tf.convert_to_tensor(intra_cluster_tensor, dtype=np.float32)\n",
    "#     inter_cluster_tensor = tf.convert_to_tensor(inter_cluster_tensor, dtype=np.float32)\n",
    "\n",
    "#     return intra_cluster_tensor, inter_cluster_tensor\n",
    "\n",
    "\n",
    "# Define the Sampling Layer\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# Define the VAE class\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, columns, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.distance_loss_tracker = keras.metrics.Mean(name=\"distance_loss\")\n",
    "\n",
    "        self.columns = columns  # Label vector to cosine similarity, dropped prior to .fit()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "            self.distance_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            converted_data: pd.DataFrame = pd.DataFrame(data.numpy(), columns=self.columns)\n",
    "            model_type = converted_data[\"model_type\"]\n",
    "            model_type = model_type.astype(int)\n",
    "            model_type = model_type_encoder.inverse_transform(model_type)\n",
    "            data = converted_data.drop(columns=[\"model_type\"])  # Drop extracted cancer model system labels\n",
    "            assert \"model_type\" not in data.columns, \"model_type should not be in data\"\n",
    "\n",
    "            cancer_type = converted_data[\"cancer_type\"]\n",
    "            cancer_type = cancer_type.astype(int)\n",
    "            cancer_type = cancer_type_encoder.inverse_transform(cancer_type)\n",
    "            data = data.drop(columns=[\"cancer_type\"]) # Drop extracted cancer type labels\n",
    "            assert \"cancer_type\" not in data.columns, \"cancer_type should not be in data\"\n",
    "\n",
    "            data = tf.convert_to_tensor(data)\n",
    "\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "\n",
    "            labeled_embeddings: pd.DataFrame = pd.DataFrame(z.numpy())\n",
    "            labeled_embeddings[\"model_type\"] = model_type\n",
    "            # print('\\n\\n\\n')\n",
    "            # print(\"Labeled Embeddings\")\n",
    "            # print(labeled_embeddings)\n",
    "            # input() # \"return break\"\n",
    "\n",
    "            # New\n",
    "            cos_sim_score = tf.keras.losses.cosine_similarity(\n",
    "                y_true, y_pred,\n",
    "                axis=-1 # ?\n",
    "                )\n",
    "\n",
    "            # Old\n",
    "            intra_cluster_distance, inter_cluster_distance = calculate_cosine_similarity(\n",
    "                df=labeled_embeddings,\n",
    "                model_type_column='model_type')\n",
    "\n",
    "            # print(\"Distances:\")\n",
    "            # print(\"Intra Cluster:\")\n",
    "            # print(intra_cluster_distance)\n",
    "            # print(\"Inter Cluster:\")\n",
    "            # print(inter_cluster_distance)\n",
    "            # input()\n",
    "\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            # Use integers / floats as coefficients, sign flips for tuning cacer model system platform correction\n",
    "            reconstruction_loss = .1 * data.shape[1] * keras.losses.binary_crossentropy(data, reconstruction)\n",
    "            kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1) # * distance_loss !!!!!!!!!  <-----\n",
    "            distance_loss = - 100 * inter_cluster_distance # absolute value, not actually back propagating\n",
    "            # intra\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss + distance_loss\n",
    "\n",
    "            # print(\"Rec Loss\")\n",
    "            # print(reconstruction_loss)\n",
    "            # print(\"kl Loss\")\n",
    "            # print(kl_loss)\n",
    "            # print(\"Distance loss\")\n",
    "            # print(distance_loss)\n",
    "            # print(\"Total loss\")\n",
    "            # print(total_loss)\n",
    "            # input()\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        self.distance_loss_tracker.update_state(distance_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            \"distance_loss\": self.distance_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "# Send distance to a dense layer\n",
    "# Build Encoder\n",
    "def build_encoder(feature_dim, latent_dim):\n",
    "    encoder_inputs = keras.Input(shape=(feature_dim,), name=\"input_1\")\n",
    "    x = keras.layers.Dense(latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_dense_1\")(encoder_inputs)\n",
    "    x = keras.layers.BatchNormalization(name=\"batchnorm\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "\n",
    "\n",
    "# Build Decoder\n",
    "def build_decoder(feature_dim, latent_dim):\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = keras.layers.Dense(feature_dim, kernel_initializer='glorot_uniform', activation='sigmoid')(latent_inputs)\n",
    "    decoder_outputs = x\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "\n",
    "# Parameters\n",
    "latent_dim = 50\n",
    "learning_rate = 0.001\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\"-f\", \"--file\", action=\"store\", type=str, required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    file: str = args.file\n",
    "\n",
    "    df = pd.read_csv(file, sep='\\t', index_col=0)\n",
    "\n",
    "    selected_df = df.iloc[:, 2:]\n",
    "\n",
    "    feature_count = selected_df.shape[1] # to encoder, decoder build\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    selected_df = pd.DataFrame(\n",
    "        scaler.fit_transform(selected_df),\n",
    "        columns=selected_df.columns,\n",
    "        index=selected_df.index)\n",
    "\n",
    "    selected_df[\"model_type\"] = df[\"model_type\"]\n",
    "    selected_df[\"cancer_type\"] = df[\"cancer_type\"]\n",
    "\n",
    "    selected_df[\"model_type\"] = model_type_encoder.fit_transform(selected_df[\"model_type\"])\n",
    "\n",
    "    selected_df[\"model_type\"] = selected_df[\"model_type\"].astype(int)\n",
    "    assert selected_df[\"model_type\"].nunique() == 2, \"There should be two classes\"\n",
    "\n",
    "    selected_df[\"cancer_type\"] = cancer_type_encoder.fit_transform(selected_df[\"cancer_type\"])\n",
    "    \n",
    "    selected_df[\"cancer_type\"] = selected_df[\"cancer_type\"].astype(int)\n",
    "\n",
    "    print(selected_df.shape)\n",
    "\n",
    "    # Build VAE\n",
    "    encoder = build_encoder(feature_count, latent_dim)  # feat count set above, lat dim is a var\n",
    "    decoder = build_decoder(feature_count, latent_dim)\n",
    "    vae = VAE(encoder, decoder, columns=selected_df.columns)\n",
    "    vae.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), run_eagerly=True)\n",
    "\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    history = vae.fit(selected_df, epochs=epochs, batch_size=batch_size, shuffle=True, callbacks=[tensorboard_callback])\n",
    "\n",
    "    # save history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_df.to_csv(Path(log_dir, \"history.tsv\", sep = '\\t'), index=False)\n",
    "\n",
    "    # save model\n",
    "    vae.save(Path(log_dir, \"model\"))\n",
    "    # Read-in trained model, to evaluation framework\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
