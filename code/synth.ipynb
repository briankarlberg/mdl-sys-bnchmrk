{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd5760b-d916-44ce-a1c3-f83ade3b309f",
   "metadata": {},
   "source": [
    "# synth url:\n",
    "https://github.com/ohsu-comp-bio/synthesis/blob/main/synth.ipynb\n",
    "\n",
    "Balance undersampled classes with synth data for mbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e6c5262-b26f-475a-82cb-21fb83aa76de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import glob as glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from random import sample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from statistics import mean\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Layer\n",
    "from tensorflow.keras import metrics, optimizers\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb8e193-7577-44de-937f-3398ddb844ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_latent(x):\n",
    "    mu, sigma = x\n",
    "    batch = K.shape(mu)[0]\n",
    "    dim = K.shape(mu)[1]\n",
    "    eps = K.random_normal(shape=(batch,dim), mean=0., stddev=1.0 )\n",
    "    return mu + K.exp(sigma/2)*eps\n",
    "\n",
    "class CustomVariationalLayer(Layer):\n",
    "    \"\"\"\n",
    "    Define a custom layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x_input, x_decoded):\n",
    "        reconstruction_loss = original_dim * metrics.binary_crossentropy(\n",
    "            x_input, x_decoded)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var_encoded - K.square(z_mean_encoded) - \n",
    "                                K.exp(z_log_var_encoded), axis=-1)\n",
    "        return K.mean(reconstruction_loss + (K.get_value(beta) * kl_loss))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded = inputs[1]\n",
    "        loss = self.vae_loss(x, x_decoded)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        return x\n",
    "    \n",
    "class WarmUpCallback(Callback):\n",
    "    def __init__(self, beta, kappa):\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if K.get_value(self.beta) <= 1:\n",
    "            K.set_value(self.beta, K.get_value(self.beta) + self.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5919a9ba-fd81-464c-9b22-d387c24d6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rndm_forest(trn, val):\n",
    "\n",
    "    val_acrcy_lst = []\n",
    "    val_running_average = []\n",
    "\n",
    "    X_trn = trn.iloc[:, 1:]\n",
    "    y_trn = trn.iloc[:, 0]\n",
    "    X_val = val.iloc[:, 1:]\n",
    "    y_val = val.iloc[:, 0]\n",
    "\n",
    "    for r in list(range(0, 10)):\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_trn, y_trn)\n",
    "        val_raw_acc = accuracy_score(y_val, clf.predict(X_val))\n",
    "        val_acrcy_lst.append(val_raw_acc)\n",
    "        val_running_average.append(mean(val_acrcy_lst))\n",
    "    return(val_running_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a3cf410-6ff4-47b7-9225-cb26c6b7006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exacloud_install_notes.txt        cptac.pdf\n",
      "\u001b[34mbeataml\u001b[m\u001b[m/                          cptac_copy_number.csv.gz\n",
      "beataml.pdf                       cptac_mutations.csv.gz\n",
      "\u001b[34mcell_line\u001b[m\u001b[m/                        cptac_proteomics.csv.gz\n",
      "cell_line.pdf                     cptac_samples.csv\n",
      "cell_line_copy_number.csv.gz      cptac_transcriptomics.csv.gz\n",
      "cell_line_drugs.tsv.gz            dta_ldr_dta.ipynb\n",
      "cell_line_experiments.tsv.gz      \u001b[34mhcmi\u001b[m\u001b[m/\n",
      "cell_line_mutations.csv.gz        hcmi.pdf\n",
      "cell_line_proteomics.csv.gz       \u001b[34mr7\u001b[m\u001b[m/\n",
      "cell_line_samples.csv             raw-data-structure-local-exa.pdf\n",
      "cell_line_transcriptomics.csv.gz  \u001b[34mtest_rel\u001b[m\u001b[m/\n",
      "\u001b[34mcptac\u001b[m\u001b[m/                            \u001b[34muni-files\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe0aca-c27b-4f7c-9b75-e6e9a0edecbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_gene_xpr.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65506fb-dfcc-449f-bd06-cdd6813427cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir i_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7acb8-73ef-4a38-b3eb-e61539275ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting sample generation and validation')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "v = 'synth_v0/'\n",
    "feature_files = 'data/*.tsv'\n",
    "cohort_n_ndx = 0 # <--- toggle test-cohort here, 0 = ACC_gene_xpr.tsv\n",
    "trn_size = 40\n",
    "pre_train_epochs = 4\n",
    "\n",
    "inpt_val = pd.DataFrame()\n",
    "dec_val = pd.DataFrame()\n",
    "synth_lat_val = pd.DataFrame()\n",
    "blend_val = pd.DataFrame()\n",
    "\n",
    "file_paths = sorted(\n",
    "    glob.glob(feature_files))\n",
    "print('Total cohorts n = ', len(file_paths))\n",
    "\n",
    "fine_tune_file = pd.read_csv(\n",
    "    file_paths[cohort_n_ndx],\n",
    "    sep = '\\t', index_col = 0)\n",
    "print(fine_tune_file.index.name)\n",
    "\n",
    "out_dirs = ['/decoded_objs/',\n",
    "            '/latent_objs/',\n",
    "            '/loss_plots/',\n",
    "            '/take-off_points/',\n",
    "            '/synthetic_sample_sets/',\n",
    "            '/rfe_out/']\n",
    "\n",
    "for out_dir in out_dirs:\n",
    "    auto_path_name = 'i_o/'+v+fine_tune_file.index.name+out_dir\n",
    "    print(auto_path_name)\n",
    "    os.makedirs(os.path.dirname(auto_path_name), exist_ok = True)\n",
    "\n",
    "pre_train_file = pd.DataFrame()\n",
    "\n",
    "file_paths.remove(\n",
    "    file_paths[cohort_n_ndx])\n",
    "\n",
    "print('Pre-train on cohorts n = ', len(file_paths))\n",
    "for path in file_paths:\n",
    "    file = pd.read_csv(path, sep = '\\t', index_col = 0)\n",
    "    pre_train_file = pd.concat( [pre_train_file, file] , axis = 0)\n",
    "\n",
    "# Each validation split constitutes an experimental replicate\n",
    "vs_list = ['vs01@','vs02@','vs03@','vs04@','vs05@','vs06@','vs07@','vs08@','vs09@','vs10@',\n",
    "           'vs11@','vs12@','vs13@','vs14@','vs15@','vs16@','vs17@','vs18@','vs19@','vs20@',\n",
    "           'vs21@','vs22@','vs23@','vs24@','vs25@']\n",
    "\n",
    "pre_train_loss_dict = {}\n",
    "fine_tune_loss_dict = {}\n",
    "for validation_split in vs_list:\n",
    "    print(validation_split)\n",
    "    val_split = validation_split + str(trn_size)\n",
    "    trn = fine_tune_file.sample(trn_size)\n",
    "    while_loop_val = 0\n",
    "    while trn.Labels.value_counts().min() < 3:\n",
    "        trn = fine_tune_file.sample(trn_size)\n",
    "        while_loop_val += 1\n",
    "        if while_loop_val == 50:\n",
    "            break\n",
    "    print('While undersampled loops: ', while_loop_val)\n",
    "    if trn.Labels.value_counts().min() < 3:\n",
    "        continue\n",
    "    val = fine_tune_file.loc[fine_tune_file[~fine_tune_file.index.isin(\n",
    "        trn.index)].index, :]    \n",
    "    inpt_val.insert(0, val_split, rndm_forest(trn, val))\n",
    "    \n",
    "    # Pre-train\n",
    "    train_file = pre_train_file\n",
    "    fit_on = str(len(pre_train_file))\n",
    "    pre_trn = 'NONE'\n",
    "    feature_set = feature_files.split('/')[-2]\n",
    "\n",
    "    fine_tune_epochs = 'NA'\n",
    "    features = train_file.columns[1:]\n",
    "\n",
    "    original_dim = len(features)\n",
    "    feature_dim = len(features)\n",
    "    latent_dim = 250\n",
    "    batch_size = 50\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(feature_dim,))\n",
    "    z_mean_dense_linear = layers.Dense(\n",
    "        latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_1\")(encoder_inputs)\n",
    "    z_mean_dense_batchnorm = layers.BatchNormalization()(z_mean_dense_linear)\n",
    "    z_mean_encoded = layers.Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "    z_log_var_dense_linear = layers.Dense(\n",
    "        latent_dim, kernel_initializer='glorot_uniform', name=\"encoder_2\")(encoder_inputs)\n",
    "    z_log_var_dense_batchnorm = layers.BatchNormalization()(z_log_var_dense_linear)\n",
    "    z_log_var_encoded = layers.Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "    latent_space = layers.Lambda(\n",
    "        compute_latent, output_shape=(\n",
    "            latent_dim,), name=\"latent_space\")([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "    decoder_to_reconstruct = layers.Dense(\n",
    "        feature_dim, kernel_initializer='glorot_uniform', activation='sigmoid')\n",
    "    decoder_outputs = decoder_to_reconstruct(latent_space)\n",
    "\n",
    "    learning_rate = 0.0005\n",
    "\n",
    "    kappa = 1\n",
    "    beta = K.variable(0)\n",
    "\n",
    "    adam = optimizers.Adam(learning_rate=learning_rate)\n",
    "    vae_layer = CustomVariationalLayer()([encoder_inputs, decoder_outputs])\n",
    "    vae = Model(encoder_inputs, vae_layer)\n",
    "    vae.compile(optimizer=adam, loss=None, loss_weights=[beta])\n",
    "\n",
    "    pre_train_epochs = pre_train_epochs\n",
    "\n",
    "    fit_start = time.time()\n",
    "    history = vae.fit(train_file.iloc[:, 1:],\n",
    "                epochs=pre_train_epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[WarmUpCallback(beta, kappa)],\n",
    "                      verbose=0)\n",
    "    pre_train_loss_dict[validation_split] = history.history['loss']\n",
    "    fit_end = time.time() - fit_start\n",
    "\n",
    "    # Fine-tune\n",
    "    train_file = trn\n",
    "    pre_trn = 'TCGA_n='+fit_on\n",
    "    fit_on = trn.index.name\n",
    "    \n",
    "    fine_tune_epochs = 150\n",
    "    \n",
    "    batch_size = 10\n",
    "    history = vae.fit(train_file.iloc[:, 1:],\n",
    "                epochs = fine_tune_epochs, batch_size=batch_size, shuffle=True,\n",
    "                callbacks = [WarmUpCallback(beta, kappa)], verbose=0)\n",
    "    fine_tune_loss_dict[validation_split] = history.history['loss']\n",
    "    \n",
    "    encoder = Model(encoder_inputs, z_mean_encoded)\n",
    "    decoder_input = keras.Input(shape = (latent_dim, ))\n",
    "    _x_decoded_mean = decoder_to_reconstruct(decoder_input)\n",
    "    decoder = Model(decoder_input, _x_decoded_mean)\n",
    "    \n",
    "    y_df = train_file.Labels\n",
    "    decoded = pd.DataFrame(decoder.predict(encoder.predict(train_file.iloc[:, 1:])),\n",
    "                           index = train_file.index, columns = train_file.iloc[:, 1:].columns)\n",
    "    \n",
    "    latent_object = pd.DataFrame(encoder.predict(train_file.iloc[:, 1:]),\n",
    "                 index=train_file.index)\n",
    "    latent_object.index.name = trn.index.name\n",
    "    latent_object = pd.concat( [pd.DataFrame(y_df), latent_object] , axis =1)\n",
    "    \n",
    "    decoded_labeled = pd.concat( [pd.DataFrame(y_df), decoded] , axis =1)\n",
    "    decoded_labeled.to_csv(\n",
    "        'i_o/'+v+'/'+fine_tune_file.index.name+'/decoded_objs/fit.'+\n",
    "        fit_on+'_epochs.'+str(fine_tune_epochs)+\n",
    "        '_pre_trained_on.'+pre_trn+'_epochs.'+str(pre_train_epochs)+\n",
    "        '_decoded_obj_latent_dim.'+str(latent_dim)+\n",
    "        '_'+feature_set+'_'+val_split+'.tsv', sep = '\\t')\n",
    "    \n",
    "    dec_val.insert(0, val_split, rndm_forest(decoded_labeled, val))\n",
    "\n",
    "    synth_full_frame = hlvs(latent_object, fine_tune_file.index.name)\n",
    "    # synth_full_frame = rnlvs(latent_object, fine_tune_file.index.name)\n",
    "    \n",
    "    synth_lat_dec = pd.concat([synth_full_frame.iloc[:, 0],\n",
    "                       pd.DataFrame(decoder.predict(synth_full_frame.iloc[:, 1:]),\n",
    "                                    index = synth_full_frame.index)], axis = 1)\n",
    "    synth_lat_dec.columns = trn.columns\n",
    "    synth_lat_dec.to_csv(\n",
    "        'i_o/'+v+fine_tune_file.index.name+'/synthetic_sample_sets/fit.'+fit_on+\n",
    "        '_epochs.'+str(fine_tune_epochs)+'_pre_trained_on.'+pre_trn+'_epochs.'+\n",
    "        str(pre_train_epochs)+'_synthetic_sample_set_latent_dim.'+str(latent_dim)+\n",
    "        '_'+feature_set+'_'+val_split+'.tsv', sep = '\\t')\n",
    "    \n",
    "    synth_lat_val.insert(0, val_split, rndm_forest(synth_lat_dec, val))\n",
    "    \n",
    "    blend = pd.concat([trn, synth_lat_dec], axis = 0)\n",
    "    blend_val.insert(0, val_split, rndm_forest(blend, val))\n",
    "    \n",
    "    blend_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                     '_blend_val.tsv', sep = '\\t')\n",
    "    \n",
    "    inpt_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                    '_input_val.tsv', sep = '\\t')\n",
    "    \n",
    "    dec_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                   '_decoded_val.tsv', sep = '\\t')\n",
    "    \n",
    "    synth_lat_val.to_csv('i_o/'+v+fine_tune_file.index.name+'/take-off_points/'+val_split+\n",
    "                   '_synth_lat_val.tsv', sep = '\\t')\n",
    "    # End experimental replicate loop\n",
    "pre_train_loss_frame = pd.DataFrame(pre_train_loss_dict)\n",
    "fine_tune_loss_frame = pd.DataFrame(fine_tune_loss_dict)\n",
    "\n",
    "pre_train_loss_frame.to_csv(\n",
    "    'i_o/'+v+'/'+fine_tune_file.index.name+'/loss_plots/fit.'+\n",
    "    fit_on+'_epochs.'+str(fine_tune_epochs)+\n",
    "    '_pre_trained_on.'+pre_trn+'_epochs.'+str(pre_train_epochs)+\n",
    "    '_pre_train_loss_vals_latent_dim.'+str(latent_dim)+\n",
    "    '_'+feature_set+'.tsv', sep = '\\t')\n",
    "\n",
    "fine_tune_loss_frame.to_csv(\n",
    "    'i_o/'+v+'/'+fine_tune_file.index.name+'/loss_plots/fit.'+\n",
    "    fit_on+'_epochs.'+str(fine_tune_epochs)+\n",
    "    '_pre_trained_on.'+pre_trn+'_epochs.'+str(pre_train_epochs)+\n",
    "    '_fine_tune_loss_vals_latent_dim.'+str(latent_dim)+\n",
    "    '_'+feature_set+'.tsv', sep = '\\t')\n",
    "\n",
    "print('main done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
